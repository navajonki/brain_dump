llm_backend: ollama
model: mistral:instruct
max_tokens: 300
use_semantic_chunking: true
temperature: 0.1
max_response_tokens: 1000
window_size: 800
overlap_size: 100
ollama_url: http://localhost:11434/api/generate 
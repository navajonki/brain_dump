llm_backend: openai
model: gpt-3.5-turbo
max_tokens: 300
use_semantic_chunking: true
temperature: 0.1
max_response_tokens: 1000
window_size: 800
overlap_size: 100 